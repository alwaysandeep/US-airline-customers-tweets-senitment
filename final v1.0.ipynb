{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the required modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (14,6)\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# SETTING THE DIRECTORY\n",
    "import os\n",
    "os.chdir(\"C:/Users/alway/OneDrive/MSBA/resume/gobble interview/Airline Customers Sentiment (Gobble walk-through)/\")\n",
    "os.getcwd()\n",
    "\n",
    "# hide all the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Loading the scikit machine learning algos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.metrics as  metrics\n",
    "from IPython.display import display\n",
    "import sys\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import  cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn import cross_validation\n",
    "import nltk.sentiment.util as vaderSentiment\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "# For exploratory data analysis\n",
    "import missingno as msno\n",
    "from  sklearn.metrics import precision_recall_fscore_support\n",
    "import emoji\n",
    "import unicodedata as uni\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#import seaborn as sns\n",
    "#from sklearn import neighbors\n",
    "#import scipy\n",
    "#from sklearn.model_selection import  cross_val_score\n",
    "#from sklearn import tree\n",
    "#import spark_sklearn.svm as svm\n",
    "#from spark_sklearn import cross_validation,tree,naive_bayes,linear_model\n",
    "#from spark_sklearn.learning_curve import learning_curve\n",
    "#from spark_sklearn.metrics import precision_recall_fscore_support,make_scorer,confusion_matrix,metrics\n",
    "#from spark_sklearn.cross_validation import cross_val_score,train_test_split\n",
    "#from sklearn.metrics import  make_scorer\n",
    "#from sklearn.svm import SVC\n",
    "#from mlxtend.feature_selection import plot_sequential_feature_selection as plot_sfs\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "#sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Defined Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def:  cost function, here the f-measure of the 'negative sentiment class'\n",
    "def my_scorer_function(y_test, y_predicted):\n",
    "    return precision_recall_fscore_support(y_test, y_predicted,pos_label=2)[2][2]\n",
    "\n",
    "#def:  this function returns the scoring object\n",
    "def two_scorer():\n",
    "    return make_scorer(my_scorer_function, greater_is_better=True)\n",
    "\n",
    "# def: this enumerates the number of times a particular emoji is repeated in the tweet ;\n",
    "        #arg: dataframe,list of emojis\n",
    "def emoji_f(df,emojis_list):\n",
    "    for i in emojis_list:\n",
    "        df[i]=df.text.str.count(str(i))\n",
    "        \"\"\"\n",
    "        for j in range(len(df)):\n",
    "            text=df.ix[j,'text']\n",
    "            df.ix[j,i]=str(text).count(str(i))\n",
    "        \"\"\"\n",
    "\n",
    "# def: this returns the confusion matrix;\n",
    "        # arg: actual target values, predicted target values \n",
    "def confustion_matrix(y_test,y_predicted):\n",
    "    confusionMatrix=confusion_matrix(y_test,y_predicted)\n",
    "    Confu_matrix_df=pd.DataFrame(confusionMatrix)\n",
    "    Confu_matrix_df.columns.name='PREDICTED'\n",
    "    Confu_matrix_df.index.name='ACTUAL'\n",
    "    Confu_matrix_df.name='CONFUSION MATRIX'\n",
    "    print \"CONFUSION MATRIX\"\n",
    "    print display(Confu_matrix_df)\n",
    "    confusionmatrix=Confu_matrix_df\n",
    "    print \"Accuracy of the model is {:.10f}\".format(metrics.accuracy_score(y_test,y_predicted))\n",
    "    precision=(100*float(confusionmatrix.ix[2,2])/(confusionmatrix.ix[2,2]+confusionmatrix.ix[0,2]+confusionmatrix.ix[1,2]))\n",
    "    recall=(100*float(confusionmatrix.ix[2,2])/(confusionmatrix.ix[2,2]+confusionmatrix.ix[2,0]+confusionmatrix.ix[2,1]))\n",
    "    print \"Precision of positive class('negative sentiment') is {:.10f}\".format(precision)\n",
    "    print \"Recall of positive class('negative sentiment') is {:.10f}\".format(recall)\n",
    "    f_measure=((2)*precision*recall)/((recall)+(precision))\n",
    "    print \"The f-measure of the model is {:.10f}\".format(f_measure)\n",
    "    return Confu_matrix_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading the tweets dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df=pd.read_csv(\"Tweets.csv\",squeeze=False)\n",
    "df.airline_sentiment.unique()\n",
    "df.head()\n",
    "# give numeric labels(0,1,2) to the sentiments(positive, neutral and negative)\n",
    "class_dict={\"positive\":0,\"neutral\":1,\"negative\":2}\n",
    "df.airline_sentiment=df.airline_sentiment.map(class_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic exploratory analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_id                            0\n",
      "airline_sentiment                   0\n",
      "airline_sentiment_confidence        0\n",
      "negativereason                   5462\n",
      "negativereason_confidence        4118\n",
      "airline                             0\n",
      "airline_sentiment_gold          14600\n",
      "name                                0\n",
      "negativereason_gold             14608\n",
      "retweet_count                       0\n",
      "text                                0\n",
      "tweet_coord                     13621\n",
      "tweet_created                       0\n",
      "tweet_location                   4733\n",
      "user_timezone                    4820\n",
      "dtype: int64\n",
      "Axes(0.125,0.125;0.775x0.755)\n",
      "airline_sentiment    0    1     2\n",
      "airline                          \n",
      "American           336  463  1960\n",
      "Delta              544  723   955\n",
      "Southwest          570  664  1186\n",
      "US Airways         269  381  2263\n",
      "United             492  697  2633\n",
      "Virgin America     152  171   181\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzkAAAGfCAYAAAB8yBzDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmSqmLJkSRJklQVS44kSZKkqvwXOyuWxW+LEMAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x168b7f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# number of missing values\n",
    "print df.isnull().sum(axis=0)\n",
    "print df.airline.value_counts().plot(kind='bar')\n",
    "print pd.crosstab(df.airline,df.airline_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emoji handling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emojis=\"😀_❤️_😁_😂_😃_😄_😅_😆_😇_😈_😉_😊_😋_😌_😍_😎_😏_😐_😑_😒_😓_😔_😕_😖_😗_😘_😙_😚_😛_😜_😝_😞_😟_😠_😡_😢_😣_😤_😥_😦_😧_😨_😩_😪_😫_😬_😭_😮_😯_😰_😱_😲_😳_😴_😵_😶_😷_😸_😹_😺_😻_😼_😽_😾_😿_🙀_🙁_🙂_🙃_🙄_🙅_🙆_🙇_🙈_🙉_🙊_🙋_🙌_🙍_🙎_🙏_🚀_🚁_🚂_🚃_🚄_🚅_🚆_🚇_🚈_🚉_🚊_🚋_🚌_🚍_🚎_🚏_🚐_🚑_🚒_🚓_🚔_🚕_🚖_🚗_🚘_🚙_🚚_🚛_🚜_🚝_🚞_🚟_🚠_🚡_🚢_🚣_🚤_🚥_🚦_🚧_🚨_🚩_🚪_🚫_🚬_🚭_🚮_🚯_🚰_🚱_🚲_🚳_🚴_🚵_🚶_🚷_🚸_🚹_🚺_🚻_🚼_🚽_🚾_🚿_🛀_🛁_🛂_🛃_🛄_🛅_🛋_🛌_🛍_🛎_🛏_🛐_🛠_🛡_🛢_🛣_🛤_🛥_🛩_🛫_🛬_🛰_🛳_🤐_🤑_🤒_🤓_🤔_🤕_🤖_🤗_🤘_🦀_🦁_🦂_🦃_🦄_🧀\"\n",
    "emojis_list=emojis.split(\"_\")\n",
    "\n",
    "# creating emoji columns \n",
    "len(df)\n",
    "df.columns\n",
    "df=df[['airline_sentiment', 'airline_sentiment_confidence',\n",
    "       'negativereason', 'negativereason_confidence', 'airline',\n",
    "       'retweet_count', 'text']]\n",
    "for i in emojis_list:\n",
    "    df[i]=0\n",
    "\n",
    "\n",
    "#emojis=\"😀_❤️_😁_😂_😃_😄_😅_😆_😇_😈_😉_😊_😋_😌_😍_😎_😏_😐_😑_😒_😓_😔_😕_😖_😗_😘_😙_😚_😛_😜_😝_😞_😟_😠_😡_😢_😣_😤_😥_😦_😧_😨_😩_😪_😫_😬_😭_😮_😯_😰_😱_😲_😳_😴_😵_😶\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# converting categorical 'airline' column into dummy variables, each variable for an unique airline\n",
    "df.airline.head()\n",
    "df1=pd.concat([df, pd.get_dummies(df.airline, prefix=None, prefix_sep='_')], axis=1, join='inner')\n",
    "df1.columns\n",
    "df=df1.ix[:,[\"airline_sentiment\",\"airline_sentiment_confidence\",\"American\",\"Delta\",\"Southwest\",\"US Airways\",\"United\",\"Virgin America\",\"retweet_count\",\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.96 s\n"
     ]
    }
   ],
   "source": [
    "# code to call emojis function\n",
    "%time emoji_f(df,emojis_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()\n",
    "# Loading vaderSentiment module changes the ouput from notebook to command prompt, \\\n",
    "        #the code below handles it        \n",
    "nb_stdout = sys.stdout\n",
    "import nltk.sentiment.util as vaderSentiment\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sys.stdout = nb_stdout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the positive and negative word dictionary corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of postive words in the corpus 2167\n",
      "number of negative words in the corpus 6297\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pos=pd.read_csv(\"Positive_Dictionary_Combined.txt\",header=None,names=['Positive_Word'],squeeze=False)\n",
    "neg=pd.read_csv(\"Negative_Dictionary_Combined.txt\",header=None,names=['Negative_Word'],squeeze=False)\n",
    "# case conversion\n",
    "pos[\"Positive_Word\"] = pos[\"Positive_Word\"].apply(lambda x: x.lower())\n",
    "neg[\"Negative_Word\"] = neg[\"Negative_Word\"].apply(lambda x: x.lower())\n",
    "# dropping the duplicate words, if any!\n",
    "pos.drop_duplicates(inplace=True)\n",
    "neg.drop_duplicates(inplace=True)\n",
    "print \"number of postive words in the corpus\",len(pos[\"Positive_Word\"])\n",
    "print \"number of negative words in the corpus\",len(neg[\"Negative_Word\"] )\n",
    "#df[\"text\"]=df[\"text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19 s\n",
      "Wall time: 43 s\n"
     ]
    }
   ],
   "source": [
    "# calculating number of unique postive/negative words present in each tweet \\\n",
    "    # and storing them in columns 'Positive_Word_Flag' & 'Negative_Word_Flag' respectively\n",
    "% time df[\"Positive_Word_Flag\"]=df[\"text\"].apply(lambda x: len([i for i in x.split(' ') if i.lower() in list(pos[\"Positive_Word\"])]))\n",
    "% time df[\"Negative_Word_Flag\"]=df[\"text\"].apply(lambda x: len([i for i in x.split(' ') if i.lower() in list(neg[\"Negative_Word\"])]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 21s\n"
     ]
    }
   ],
   "source": [
    "# Calculating the polarity scores (which comes as a dictionary object) in the tweets using vaderSentiment package and \\\n",
    "    # *storing them in 'Sentiment' column\n",
    "%time df[\"Sentiment\"] = df[\"text\"].apply(lambda line: SentimentIntensityAnalyzer().polarity_scores(line))\n",
    "\n",
    "# Extracting the negative, neutral, positive and compund polairty scores from the 'Sentiment' column\n",
    "df[[\"Vader_compound\",\"Vader_neg\",\"Vader_neu\",\"Vader_pos\"]] = df[\"Sentiment\"].apply(pd.Series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Creating column 'Uppercase', which contains the number of specific range upper case words in tweets\n",
    "df[\"Uppercase\"] = np.nan\n",
    "df[\"Uppercase\"] = df[\"text\"].apply(lambda line: [np.sum([1 if re.search('[A-Z]{5,}',word) is not None else 0]) for word in line.split(' ')])\n",
    "df[\"Uppercase\"] = df[\"Uppercase\"].apply(lambda x: np.sum( x))\n",
    "\n",
    "# Creating 'Special_Characters' column, which contains the number of times [!!* | ??*] appears in the tweet\n",
    "df[\"Special_Characters\"] = np.nan\n",
    "#handling  http://, https://, or use \\W\n",
    "df[\"Special_Characters\"] = df[\"text\"].apply(lambda line: [np.sum([1 if re.search('[\\!\\?\\.]{2,}',word.replace(\"https://\",\"\").replace(\"http://\",\"\")) is not None else 0]) for word in line.split(' ')])\n",
    "df[\"Special_Characters\"] = df[\"Special_Characters\"].apply(lambda x: np.sum(x))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COME BACK TO THE BELOW CODE AND FIX IT!!!!!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"Vader_compound\"] = df[\"Vader_compound\"].apply(lambda x: abs(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# converting text to numeric TF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X=df.ix[:,1:]\n",
    "\n",
    "y=df.ix[:,[0]]\n",
    "\n",
    "# train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y , test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## below code handles --- stemming,  tokenizing, stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "stemmer = EnglishStemmer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "\n",
    "# Removing numeric characters\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc) if ((len(re.findall(pattern=r'[0-9]',string=w))==0) and len(w)>1))\n",
    "\n",
    "# v is for one gram tokenizer and v1 is for 2-4 ngram tokenizer\n",
    "\n",
    "v1=CountVectorizer(stop_words ='english',max_df=.5,min_df=2,encoding =\"utf-8\",lowercase =True,ngram_range=(2, 4))\n",
    "v=CountVectorizer(stop_words ='english',max_df=.5,min_df=2,encoding =\"utf-8\",lowercase =True,analyzer=stemmed_words,token_pattern =r'[a-zA-Z]{3,}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding='utf-8', input=u'content',\n",
       "        lowercase=True, max_df=0.5, max_features=None, min_df=2,\n",
       "        ngram_range=(2, 4), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the one gram and 2-4 ngram tokenizer\n",
    "v.fit(X_train.text)\n",
    "v1.fit(X_train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transforming the text using the fitted tokenizer\n",
    "X_train_text_TF1=v.transform(X_train.text)\n",
    "X_test_text_TF1=v.transform(X_test.text)\n",
    "X_train_text_TF2=v1.transform(X_train.text)\n",
    "X_test_text_TF2=v1.transform(X_test.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2928L, 13377L)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train_text_TF1.toarray().shape\n",
    "#X_test_text_TF1.toarray().shape\n",
    "#X_train_text_TF2.toarray().shape\n",
    "X_test_text_TF2.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of columns in train  17378\n"
     ]
    }
   ],
   "source": [
    "#len(v.get_feature_names())\n",
    "#len(v1.get_feature_names())\n",
    "\n",
    "# Creating the dataframe using the matrix of tokens from countvectorizer with its features(unique tokens)\n",
    "df_train_newX1=pd.DataFrame(X_train_text_TF1.toarray(), columns=v.get_feature_names(),index=X_train.index)\n",
    "df_train_newX2=pd.DataFrame(X_train_text_TF2.toarray(), columns=v1.get_feature_names(),index=X_train.index)\n",
    "# merging one gram and ngram features in one dataframe\n",
    "df_train_newX=df_train_newX1.merge(df_train_newX2,left_index=True,right_index=True)\n",
    "#len(df_train_newX1.columns)\n",
    "#len(df_train_newX2.columns)\n",
    "print \"number of columns in train \",len(df_train_newX.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of columns in test  17378\n"
     ]
    }
   ],
   "source": [
    "# Doing the same operation similarily for test dataset\n",
    "df_test_newX1=pd.DataFrame(X_test_text_TF1.toarray(), columns=v.get_feature_names(),index=X_test.index)\n",
    "#df_test_newX1.head()\n",
    "df_test_newX2=pd.DataFrame(X_test_text_TF2.toarray(), columns=v1.get_feature_names(),index=X_test.index)\n",
    "#df_test_newX2.head()\n",
    "df_test_newX=df_test_newX1.merge(df_test_newX2,left_index=True,right_index=True)\n",
    "print \"number of columns in test \",len(df_test_newX.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merging the TF of tokens features to orginal data frames with manual engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 'manual1' columns contains whether there is a positive and negative word\n",
    "\n",
    "X_train['manual1']=((X_train.Positive_Word_Flag >1 ) & (X_train.Negative_Word_Flag >0 ))\n",
    "X_test['manual1']=((X_test.Positive_Word_Flag >1 ) & (X_test.Negative_Word_Flag >0 ))\n",
    "\n",
    "words=['thanks','thank','?']\n",
    "pat = '|'.join(map(re.escape, words))\n",
    "\n",
    "# 'manual2' columns contains whether there is a 'thanks' or 'thank' word and atleast one negative word\n",
    "X_train['manual2']=((X_train.text.str.contains(pat) ) & (X_train.Negative_Word_Flag >0 ))\n",
    "X_test['manual12']=((X_test.text.str.contains(pat) ) & (X_test.Negative_Word_Flag >0 ))\n",
    "\n",
    "# Merging into one complete train, test dataframe\n",
    "X_train_new = pd.concat([X_train, df_train_newX], axis=1, join='inner')\n",
    "X_test_new = pd.concat([X_test, df_test_newX], axis=1, join='inner')\n",
    "\n",
    "\n",
    "# Dropping the 'text' column from both train and test dfs\n",
    "X_train_new=X_train_new.drop(\"text\",axis=1)\n",
    "X_test_new=X_test_new.drop(\"text\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>American</th>\n",
       "      <th>Delta</th>\n",
       "      <th>Southwest</th>\n",
       "      <th>US Airways</th>\n",
       "      <th>United</th>\n",
       "      <th>Virgin America</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>😀</th>\n",
       "      <th>❤️</th>\n",
       "      <th>...</th>\n",
       "      <th>yup new beanie</th>\n",
       "      <th>yup new beanie http</th>\n",
       "      <th>yyz terminal</th>\n",
       "      <th>zero communication</th>\n",
       "      <th>zero entertainment</th>\n",
       "      <th>zero information</th>\n",
       "      <th>zero response</th>\n",
       "      <th>zkatcher bretharold</th>\n",
       "      <th>zone precious</th>\n",
       "      <th>zone precious gems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>0.6737</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6875</th>\n",
       "      <td>0.6757</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7598</th>\n",
       "      <td>0.6671</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14124</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6187</th>\n",
       "      <td>0.6883</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 17579 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       airline_sentiment_confidence  American  Delta  Southwest  US Airways  \\\n",
       "750                          0.6737         0      0          0           0   \n",
       "6875                         0.6757         0      1          0           0   \n",
       "7598                         0.6671         0      1          0           0   \n",
       "14124                        1.0000         1      0          0           0   \n",
       "6187                         0.6883         0      0          1           0   \n",
       "\n",
       "       United  Virgin America  retweet_count  😀  ❤️         ...          \\\n",
       "750         1               0              0   0   0         ...           \n",
       "6875        0               0              0   0   0         ...           \n",
       "7598        0               0              0   0   0         ...           \n",
       "14124       0               0              0   0   0         ...           \n",
       "6187        0               0              0   0   0         ...           \n",
       "\n",
       "       yup new beanie  yup new beanie http  yyz terminal  zero communication  \\\n",
       "750                 0                    0             0                   0   \n",
       "6875                0                    0             0                   0   \n",
       "7598                0                    0             0                   0   \n",
       "14124               0                    0             0                   0   \n",
       "6187                0                    0             0                   0   \n",
       "\n",
       "       zero entertainment  zero information  zero response  \\\n",
       "750                     0                 0              0   \n",
       "6875                    0                 0              0   \n",
       "7598                    0                 0              0   \n",
       "14124                   0                 0              0   \n",
       "6187                    0                 0              0   \n",
       "\n",
       "       zkatcher bretharold  zone precious  zone precious gems  \n",
       "750                      0              0                   0  \n",
       "6875                     0              0                   0  \n",
       "7598                     0              0                   0  \n",
       "14124                    0              0                   0  \n",
       "6187                     0              0                   0  \n",
       "\n",
       "[5 rows x 17579 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_new.Sentiment.head()\n",
    "# Dropping 'Sentiment' column from both train and test dataframe\n",
    "X_train_new.drop([\"Sentiment\"], \n",
    "        axis=1, inplace=True)\n",
    "X_test_new.drop([\"Sentiment\"], \n",
    "        axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(X_train_new.columns)\n",
    "\n",
    "# Initialize the class\n",
    "clf1 = MultinomialNB()\n",
    "\n",
    "# Fitting the model\n",
    "%time  clf1.fit(X_train_new, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make class predictions on  test data\n",
    "y_pred_class = clf1.predict(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80669398907103829"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy of class predictions\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>PREDICTED</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACTUAL</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>308</td>\n",
       "      <td>32</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>243</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>54</td>\n",
       "      <td>1811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "PREDICTED    0    1     2\n",
       "ACTUAL                   \n",
       "0          308   32   119\n",
       "1           41  243   296\n",
       "2           24   54  1811"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Accuracy of the model is 0.8066939891\n",
      "Precision of positive class('negative sentiment') is 81.3566936208\n",
      "Recall of positive class('negative sentiment') is 95.8708311276\n",
      "The f-measure of the model is 88.0194410693\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Getting the confusion matrix by passing actual classes and predicted classes    \n",
    "df_confusion=confustion_matrix(y_test,y_pred_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.74038462,  0.53465347,  0.88019441])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use this code for just f-measure\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_test,y_pred_class,pos_label=2)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets that are negative and are classified as positive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2928"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analyze_prediction=pd.concat([X_test.text, y_test,pd.DataFrame(y_pred_class,index=X_test.index)], axis=1, join='inner')\n",
    "df_analyze_prediction.columns=[\"text\",\"actual_sentiment\",\"predicted_sentiment\"]\n",
    "df_analyze_prediction.head(2)\n",
    "df_analyze_prediction.predicted_sentiment.value_counts()\n",
    "len(df_analyze_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ '@united thanks for having ground crews that are surprised when flights arrive. #beingsuckontarmacsucks!',\n",
       "       '@united thanks for the reply. To clarify, the airfare is similar to your likely intended peer group. The $3 beer charge, however, is not',\n",
       "       '@USAirways suggest you failures make a HUGE donation to @the_USO  Charlotte, NC as THEY provided GREAT customer service today, unlike you.',\n",
       "       '&lt;3 &lt;3 RT @SouthwestAir! @danihampton Sorry to hear about the WiFi connection, Dani. Please DM us your conf # so we can help you. Thanks!',\n",
       "       \"@AmericanAir a confirmed flight. I'm so done! Thanks for nothing!\",\n",
       "       '@united nice. I wonder how you pick who to respond to? Maybe only happy customers are easier. I wonder how many of those you have left.',\n",
       "       \"\\xe2\\x80\\x9c@AmericanAir: @Dumas2TTG Good morning, Tamara. We'll try to get you comfortably on a flight as soon as we can.\\xe2\\x80\\x9d#NoXTraLegRoom #NoCoatCloset\",\n",
       "       '@USAirways No thank you. @AmericanAir was responsive &amp; I found alternate travel home.',\n",
       "       \"@AmericanAir Thank you for the acknowledgement. The IFE didn't work all that well anyway so maybe time to upgrade to lower profile system.\",\n",
       "       '@United: I feel very well informed. yes, the flight is indeed delayed. Thank you. http://t.co/FuCHZRZjg5',\n",
       "       '@USAirways thank you for reaching out to me. Hopefully my request will be fixed and I can remain a loyal customer.',\n",
       "       '@SouthwestAir you do a great job of achieving that at most cities, just not at BWI.',\n",
       "       '@united cross country flight SFO&gt;BOS. No wifi, not even a can of soda and this quality inflight entertainment #sad http://t.co/xhlc30MTfF',\n",
       "       '@united i think he actually did not like your screen  @campilley \\xf0\\x9f\\x98\\x83\\xf0\\x9f\\x98\\x83\\xf0\\x9f\\x98\\x83',\n",
       "       '@SouthwestAir Redeem-points packages magically gone by time we finally got through @ 10:01, felt like #baitandswitch',\n",
       "       '@JetBlue but you guys should know that musicians are very sensitive about the safety of their instruments when flying. For good reason.',\n",
       "       \"@SouthwestAir finally boarded. Looks like I'll make it to my final destination but my baggage won't # baggagefail #bagsflyfreebutnotwithme\",\n",
       "       \"@VirginAmerica Can I get some help with a support ticket? It's been 15 days.... Incident: 150202-000419 Thank you!\",\n",
       "       '@united Dmed you. Thank you',\n",
       "       '@JetBlue the whole plane. Hoping for better luck on the return flight Sunday.',\n",
       "       \"@VirginAmerica Just trying to book tickets to NYC and facing super fun broken styling. Don't worry, I'll keep trying :)\",\n",
       "       '@USAirways Thanks for showing interest...3880 to Greenville, SC.',\n",
       "       '@united It is all good -- your 1K team has been nothing short of amazing with everything.',\n",
       "       '@VirginAmerica Your chat support is not working on your site: http://t.co/vhp2GtDWPk'], dtype=object)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=df_analyze_prediction.ix[((df_analyze_prediction.actual_sentiment==2) & (df_analyze_prediction.predicted_sentiment==0)),\"text\"]\n",
    "len(x)\n",
    "x.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets that are negative and are classified as neutral "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'Includes @KCIAirport \\xe2\\x80\\x9c@SouthwestAir: Inclement weather may impact scheduled service in. ReFlight Booking Problems options available: http://t.co/KeyrpFlHil\\xe2\\x80\\x9d',\n",
       "       '@JetBlue it seems I never received an ID, just a password.',\n",
       "       '@united how come a $27 shuttle bus from LGA to EWR has electrical power outlets but our new Airbus interiors do not? #whyjeff?',\n",
       "       '@JetBlue your gif game is strong.',\n",
       "       \"@JetBlue looks like their inflatable car seats got left on the plane, so I guess they'll be back sometime!! #savethoseseats\",\n",
       "       '@united Club DEN, East or West, both are disgusting. http://t.co/XijYrPsLZK',\n",
       "       '@united Not a happy flyer.  UA flight 1161 from SFO to Cleveland.  Sunday 2/22.  Stay tuned for blog article from http://t.co/VdFdODqVGx',\n",
       "       \"@SouthwestAir looks like you are up and running for the day. Still can't get through on the phone. Can you follow so I can dm you my info\",\n",
       "       \"\\xe2\\x80\\x9c@JetBlue: Our fleet's on fleek. http://t.co/7iM9rHIvyR\\xe2\\x80\\x9d Plz stop\",\n",
       "       \"We didn't need this. RT @JetBlue: Our fleet's on fleek. http://t.co/cpZB285o71\",\n",
       "       '@SouthwestAir Yes - with extra $77. I wonder what are you going yo loose, if there is room.',\n",
       "       \"@AmericanAir attached original ticket. flight number was 1672. isn't the experience I thought I would have. Terrible! http://t.co/5SdlyN9MSS\",\n",
       "       \"\\xe2\\x80\\x9c@JetBlue: Our fleet's on fleek. http://t.co/5xGnyHGAFz\\xe2\\x80\\x9d this is what I hate\",\n",
       "       '@SouthwestAir I left my iPad on my flight 1831 from ERW to DEN. I opened claim 442998. It has so many memories on it. Plz help get it back.',\n",
       "       \"STOP. USING.THIS.WORD. IF. YOU'RE. A. COMPANY. RT @JetBlue: Our fleet's on fleek. http://t.co/Fd2TNYcTrB\",\n",
       "       \"@AmericanAir I am trying to add my tsa pre check number to my reservation online and don't see an option to do that..?? #nosecuritylines\",\n",
       "       '@united what incentive do I have to give you another chance?',\n",
       "       \"@VirginAmerica FYI the info@virginamerica.com email address you say to contact in password reset emails doesn't exist. Emails bounce.\",\n",
       "       '@SouthwestAir same here. Would appreciate a follow so I can DM my info to figure out what I am supposed to do.',\n",
       "       '@USAirways lol thanks to you we had to switch to American http://t.co/Sl6BDRXfN8',\n",
       "       '@united There are Exit-Row window shades in approx 11% of the windows  of your RJ-145s. &lt;--a frequent flyer http://t.co/aOEaeszDLX',\n",
       "       '@VirginAmerica I was really looking forward to my flight. can you let me know when it will be rescheduled? #diehardvirgin',\n",
       "       '@SouthwestAir when will you be accepting reservations past August 7?',\n",
       "       \"@SouthwestAir how long does it take for points from RR Shopping to post to my account? Got email saying I've earned points but don't see em\",\n",
       "       '@united correct date is 2/11/15!',\n",
       "       \"@JetBlue I just booked a flight with you guys. I'm reconsidering that decision now after reading this tweet.\",\n",
       "       \"@SouthwestAir Why doesn't mean TSA PreCheck show up on my mobile boarding pass? My KTN is linked to my account.\",\n",
       "       '@SouthwestAir Just landed in PHL. Row 9 window cover on N366SW could use some LUV. http://t.co/WQZZtIemX0',\n",
       "       \"@united MR, she's on her way now, but thought id detail the extravaganza for you... #dobetter #please http://t.co/V8PVpHMtZc\",\n",
       "       \"@SouthwestAir TSA Pre isn't showing up on my boarding passes and I've followed all of the steps. Please help.\",\n",
       "       '@JetBlue 290 to Boston',\n",
       "       '.@united what link? Those are all the email offers u sent me in a year to buy/transfer miles.The last 1 just expired http://t.co/ZZPs5ywVE2',\n",
       "       'Stop the madness \"@JetBlue: Our fleet\\'s on fleek. http://t.co/Q5a7jtkI5K\\xe2\\x80\\x9d',\n",
       "       \"@JetBlue \\xf0\\x9f\\x91\\x8d. I think I'll come take a nap at the terminal.  ;)\",\n",
       "       '@SouthwestAir will you please start offering services out of @triflight and @AshevilleAir to other airports.. Have to drive 3-4 hrs to use u',\n",
       "       '@united you have to follow me in order for me to DM...come on now',\n",
       "       \"@virginamerica how's a direct flight FLL-&gt;SFO have unexpected layover in Vegas 4 fuel yet peeps next to me bought for Vegas flight. #sneaky\",\n",
       "       \"See what you started now @nytimes RT @JetBlue: Our fleet's on fleek. http://t.co/atd2Sm8HF4\",\n",
       "       \"@AmericanAir I DM'd you. Anything you can do?\",\n",
       "       \"@USAirways Well, that's a problem considering I need to book a flight tonight for this weekend... is there anything you can do?\",\n",
       "       \"@united can't DM, you're not following. LLY144. Rebooked on #UA1516 but still need seats.\",\n",
       "       '@JetBlue if u want to be helpful save me the trip from Westchester Cty to JFK, plus parking - my info is in the photo http://t.co/JI7xkS2jkk',\n",
       "       \"No JetBlue\\xe2\\x80\\xa6 Just\\xe2\\x80\\xa6 no \\xe2\\x80\\x9c@JetBlue: Our fleet's on fleek. http://t.co/mB3IPGs8zQ\\xe2\\x80\\x9d\",\n",
       "       \"@SouthwestAir I'm stuck in Fort Lauderdale.\",\n",
       "       '@VirginAmerica can\\xe2\\x80\\x99t access your website from Safari on iPhone 6. Seems to work on Mac and iPad. Need iPhone to add Passbook.',\n",
       "       '@JetBlue *cough* #awkward!',\n",
       "       \"Sir. RT @JetBlue: Our fleet's on fleek. http://t.co/F5IXyw8XyB\",\n",
       "       \"@JetBlue they booked me for tomorrow. It's $250 higher. My 1st reservation is PGFRYZ. Get me on standby list 4 tonight would help.\",\n",
       "       \"@united's first-class #cockup\\nhttp://t.co/oh7CFv7DHR\",\n",
       "       '@united this is in ur Hemispheres magazine. I\\'m open to what will u do to make the \"flight more pleasant.\" http://t.co/VrQDpqEPFW',\n",
       "       '.@united too much info to share via tweet. Please send me your name and contact info. Happy to supply you with images and CS rep names.',\n",
       "       '@AmericanAir Best planes in the business. NOT? AA5411 today. http://t.co/I7Vdi9WqSF',\n",
       "       '@AmericanAir just look at RDU airport. please think about the safety of your passengers. we cant get to RDU safely!\\n\\nhttp://t.co/wDyEkVB1Ze',\n",
       "       \"@united wont transfer flight ticket to accompany an 11 yr old who's active military mom had to have emergency brain surgery? WOW!!\"], dtype=object)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=df_analyze_prediction.ix[((df_analyze_prediction.actual_sentiment==2) & (df_analyze_prediction.predicted_sentiment==1)),\"text\"]\n",
    "\n",
    "x.values"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
